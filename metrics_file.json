[{"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "730", "f-measure": "0.576982097187", "recall": "0.853252647504", "precision": "0.435857805255", "tp": "564", "tn": "1533", "fn": "97", "accuracy": "0.717168262654"}}, {"genre": "MPB", "metrics": {"fp": "132", "f-measure": "0.534195933457", "recall": "0.43721633888", "precision": "0.686460807601", "tp": "289", "tn": "1808", "fn": "372", "accuracy": "0.806228373702"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "124", "f-measure": "0.668363019508", "recall": "0.596066565809", "precision": "0.760617760618", "tp": "394", "tn": "1703", "fn": "267", "accuracy": "0.842845659164"}}, {"genre": "Funk Carioca", "metrics": {"fp": "76", "f-measure": "0.82362330407", "recall": "0.780635400908", "precision": "0.871621621622", "tp": "516", "tn": "1581", "fn": "145", "accuracy": "0.904659188956"}}, {"genre": "Samba", "metrics": {"fp": "146", "f-measure": "0.585451358457", "recall": "0.505295007564", "precision": "0.695833333333", "tp": "334", "tn": "1763", "fn": "327", "accuracy": "0.815953307393"}}], "classifier_metrics": [{"micro": {"fp": "1208.0", "f-measure": "0.634493192133", "recall": "0.634493192133", "precision": "0.634493192133", "tp": "2097.0", "tn": "8388.0", "fn": "1208.0", "accuracy": "0.812727695527"}}, {"macro": {"fp": "241.6", "f-measure": "0.637723142536", "recall": "0.634493192133", "precision": "0.690078265686", "tp": "419.4", "tn": "1677.6", "fn": "241.6", "accuracy": "0.817370958374"}}, {"general": {"accuracy": 0.6344931921331316}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "True", "stemming": "True", "remove_stopwords": "True", "test_number": "1"}}]