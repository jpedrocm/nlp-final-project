[{"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "670", "f-measure": "0.58854718982", "recall": "0.839636913767", "precision": "0.45306122449", "tp": "555", "tn": "1974", "fn": "106", "accuracy": "0.765204236006"}}, {"genre": "MPB", "metrics": {"fp": "144", "f-measure": "0.539019963702", "recall": "0.449319213313", "precision": "0.673469387755", "tp": "297", "tn": "2500", "fn": "364", "accuracy": "0.846293494705"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "164", "f-measure": "0.657445077299", "recall": "0.61119515885", "precision": "0.711267605634", "tp": "404", "tn": "2480", "fn": "257", "accuracy": "0.872617246596"}}, {"genre": "Funk Carioca", "metrics": {"fp": "82", "f-measure": "0.837245696401", "recall": "0.809379727685", "precision": "0.867098865478", "tp": "535", "tn": "2562", "fn": "126", "accuracy": "0.93706505295"}}, {"genre": "Samba", "metrics": {"fp": "157", "f-measure": "0.532735426009", "recall": "0.449319213313", "precision": "0.654185022026", "tp": "297", "tn": "2487", "fn": "364", "accuracy": "0.842360060514"}}], "classifier_metrics": [{"macro": {"fp": "243.4", "f-measure": "0.630998670646", "recall": "0.631770045386", "precision": "0.671816421077", "tp": "417.6", "tn": "2400.6", "fn": "243.4", "accuracy": "0.852708018154"}}, {"micro": {"fp": "1217.0", "f-measure": "0.631770045386", "recall": "0.631770045386", "precision": "0.631770045386", "tp": "2088.0", "tn": "12003.0", "fn": "1217.0", "accuracy": "0.852708018154"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "True", "stemming": "True", "remove_stopwords": "True", "test_number": "1"}}, {"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "682", "f-measure": "0.574309978769", "recall": "0.81845688351", "precision": "0.442354865086", "tp": "541", "tn": "1962", "fn": "120", "accuracy": "0.757337367625"}}, {"genre": "MPB", "metrics": {"fp": "189", "f-measure": "0.525585429315", "recall": "0.458396369138", "precision": "0.615853658537", "tp": "303", "tn": "2455", "fn": "358", "accuracy": "0.834493192133"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "146", "f-measure": "0.640269587195", "recall": "0.574886535552", "precision": "0.722433460076", "tp": "380", "tn": "2498", "fn": "281", "accuracy": "0.870801815431"}}, {"genre": "Funk Carioca", "metrics": {"fp": "87", "f-measure": "0.808917197452", "recall": "0.768532526475", "precision": "0.853781512605", "tp": "508", "tn": "2557", "fn": "153", "accuracy": "0.927382753404"}}, {"genre": "Samba", "metrics": {"fp": "173", "f-measure": "0.52389380531", "recall": "0.447806354009", "precision": "0.631130063966", "tp": "296", "tn": "2471", "fn": "365", "accuracy": "0.83721633888"}}], "classifier_metrics": [{"macro": {"fp": "255.4", "f-measure": "0.614595199608", "recall": "0.613615733737", "precision": "0.653110712054", "tp": "405.6", "tn": "2388.6", "fn": "255.4", "accuracy": "0.845446293495"}}, {"micro": {"fp": "1277.0", "f-measure": "0.613615733737", "recall": "0.613615733737", "precision": "0.613615733737", "tp": "2028.0", "tn": "11943.0", "fn": "1277.0", "accuracy": "0.845446293495"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "TF", "case-folding": "True", "stemming": "True", "remove_stopwords": "True", "test_number": "2"}}]