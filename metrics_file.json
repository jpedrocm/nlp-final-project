[{"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "707", "f-measure": "0.586046511628", "recall": "0.857791225416", "precision": "0.445054945055", "tp": "567", "tn": "1937", "fn": "94", "accuracy": "0.757639939486"}}, {"genre": "MPB", "metrics": {"fp": "144", "f-measure": "0.561215370867", "recall": "0.475037821483", "precision": "0.685589519651", "tp": "314", "tn": "2500", "fn": "347", "accuracy": "0.851437216339"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "146", "f-measure": "0.652754590985", "recall": "0.591527987897", "precision": "0.728119180633", "tp": "391", "tn": "2498", "fn": "270", "accuracy": "0.8741301059"}}, {"genre": "Funk Carioca", "metrics": {"fp": "60", "f-measure": "0.834276475344", "recall": "0.780635400908", "precision": "0.895833333333", "tp": "516", "tn": "2584", "fn": "145", "accuracy": "0.937972768533"}}, {"genre": "Samba", "metrics": {"fp": "138", "f-measure": "0.57448706512", "recall": "0.487140695915", "precision": "0.7", "tp": "322", "tn": "2506", "fn": "339", "accuracy": "0.85567322239"}}], "classifier_metrics": [{"macro": {"fp": "239.0", "f-measure": "0.641756002789", "recall": "0.638426626324", "precision": "0.690919395734", "tp": "422.0", "tn": "2405.0", "fn": "239.0", "accuracy": "0.85537065053"}}, {"micro": {"fp": "1195.0", "f-measure": "0.638426626324", "recall": "0.638426626324", "precision": "0.638426626324", "tp": "2110.0", "tn": "12025.0", "fn": "1195.0", "accuracy": "0.85537065053"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "True", "stemming": "True", "remove_stopwords": "True", "test_number": "1"}}, {"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "721", "f-measure": "0.582564102564", "recall": "0.85930408472", "precision": "0.44065166796", "tp": "568", "tn": "1923", "fn": "93", "accuracy": "0.753706505295"}}, {"genre": "MPB", "metrics": {"fp": "147", "f-measure": "0.567375886525", "recall": "0.484114977307", "precision": "0.6852248394", "tp": "320", "tn": "2497", "fn": "341", "accuracy": "0.852344931921"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "139", "f-measure": "0.658843252305", "recall": "0.594553706505", "precision": "0.738721804511", "tp": "393", "tn": "2505", "fn": "268", "accuracy": "0.876853252648"}}, {"genre": "Funk Carioca", "metrics": {"fp": "50", "f-measure": "0.838235294118", "recall": "0.776096822995", "precision": "0.911190053286", "tp": "513", "tn": "2594", "fn": "148", "accuracy": "0.940090771558"}}, {"genre": "Samba", "metrics": {"fp": "132", "f-measure": "0.577578475336", "recall": "0.487140695915", "precision": "0.709251101322", "tp": "322", "tn": "2512", "fn": "339", "accuracy": "0.857488653555"}}], "classifier_metrics": [{"macro": {"fp": "237.8", "f-measure": "0.64491940217", "recall": "0.640242057489", "precision": "0.697007893296", "tp": "423.2", "tn": "2406.2", "fn": "237.8", "accuracy": "0.856096822995"}}, {"micro": {"fp": "1189.0", "f-measure": "0.640242057489", "recall": "0.640242057489", "precision": "0.640242057489", "tp": "2116.0", "tn": "12031.0", "fn": "1189.0", "accuracy": "0.856096822995"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "True", "stemming": "True", "remove_stopwords": "False", "test_number": "2"}}, {"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "707", "f-measure": "0.586046511628", "recall": "0.857791225416", "precision": "0.445054945055", "tp": "567", "tn": "1937", "fn": "94", "accuracy": "0.757639939486"}}, {"genre": "MPB", "metrics": {"fp": "144", "f-measure": "0.561215370867", "recall": "0.475037821483", "precision": "0.685589519651", "tp": "314", "tn": "2500", "fn": "347", "accuracy": "0.851437216339"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "146", "f-measure": "0.652754590985", "recall": "0.591527987897", "precision": "0.728119180633", "tp": "391", "tn": "2498", "fn": "270", "accuracy": "0.8741301059"}}, {"genre": "Funk Carioca", "metrics": {"fp": "60", "f-measure": "0.834276475344", "recall": "0.780635400908", "precision": "0.895833333333", "tp": "516", "tn": "2584", "fn": "145", "accuracy": "0.937972768533"}}, {"genre": "Samba", "metrics": {"fp": "138", "f-measure": "0.57448706512", "recall": "0.487140695915", "precision": "0.7", "tp": "322", "tn": "2506", "fn": "339", "accuracy": "0.85567322239"}}], "classifier_metrics": [{"macro": {"fp": "239.0", "f-measure": "0.641756002789", "recall": "0.638426626324", "precision": "0.690919395734", "tp": "422.0", "tn": "2405.0", "fn": "239.0", "accuracy": "0.85537065053"}}, {"micro": {"fp": "1195.0", "f-measure": "0.638426626324", "recall": "0.638426626324", "precision": "0.638426626324", "tp": "2110.0", "tn": "12025.0", "fn": "1195.0", "accuracy": "0.85537065053"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "False", "stemming": "True", "remove_stopwords": "True", "test_number": "3"}}, {"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "721", "f-measure": "0.582564102564", "recall": "0.85930408472", "precision": "0.44065166796", "tp": "568", "tn": "1923", "fn": "93", "accuracy": "0.753706505295"}}, {"genre": "MPB", "metrics": {"fp": "147", "f-measure": "0.567375886525", "recall": "0.484114977307", "precision": "0.6852248394", "tp": "320", "tn": "2497", "fn": "341", "accuracy": "0.852344931921"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "139", "f-measure": "0.658843252305", "recall": "0.594553706505", "precision": "0.738721804511", "tp": "393", "tn": "2505", "fn": "268", "accuracy": "0.876853252648"}}, {"genre": "Funk Carioca", "metrics": {"fp": "50", "f-measure": "0.838235294118", "recall": "0.776096822995", "precision": "0.911190053286", "tp": "513", "tn": "2594", "fn": "148", "accuracy": "0.940090771558"}}, {"genre": "Samba", "metrics": {"fp": "132", "f-measure": "0.577578475336", "recall": "0.487140695915", "precision": "0.709251101322", "tp": "322", "tn": "2512", "fn": "339", "accuracy": "0.857488653555"}}], "classifier_metrics": [{"macro": {"fp": "237.8", "f-measure": "0.64491940217", "recall": "0.640242057489", "precision": "0.697007893296", "tp": "423.2", "tn": "2406.2", "fn": "237.8", "accuracy": "0.856096822995"}}, {"micro": {"fp": "1189.0", "f-measure": "0.640242057489", "recall": "0.640242057489", "precision": "0.640242057489", "tp": "2116.0", "tn": "12031.0", "fn": "1189.0", "accuracy": "0.856096822995"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "False", "stemming": "True", "remove_stopwords": "False", "test_number": "4"}}, {"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "711", "f-measure": "0.591375770021", "recall": "0.871406959153", "precision": "0.447552447552", "tp": "576", "tn": "1933", "fn": "85", "accuracy": "0.75915279879"}}, {"genre": "MPB", "metrics": {"fp": "136", "f-measure": "0.55877034358", "recall": "0.467473524962", "precision": "0.694382022472", "tp": "309", "tn": "2508", "fn": "352", "accuracy": "0.852344931921"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "137", "f-measure": "0.668890742285", "recall": "0.606656580938", "precision": "0.745353159851", "tp": "401", "tn": "2507", "fn": "260", "accuracy": "0.879878971256"}}, {"genre": "Funk Carioca", "metrics": {"fp": "54", "f-measure": "0.848631239936", "recall": "0.797276853253", "precision": "0.907056798623", "tp": "527", "tn": "2590", "fn": "134", "accuracy": "0.943116490166"}}, {"genre": "Samba", "metrics": {"fp": "126", "f-measure": "0.588340807175", "recall": "0.49621785174", "precision": "0.722466960352", "tp": "328", "tn": "2518", "fn": "333", "accuracy": "0.861119515885"}}], "classifier_metrics": [{"macro": {"fp": "232.8", "f-measure": "0.651201780599", "recall": "0.647806354009", "precision": "0.70336227777", "tp": "428.2", "tn": "2411.2", "fn": "232.8", "accuracy": "0.859122541604"}}, {"micro": {"fp": "1164.0", "f-measure": "0.647806354009", "recall": "0.647806354009", "precision": "0.647806354009", "tp": "2141.0", "tn": "12056.0", "fn": "1164.0", "accuracy": "0.859122541604"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "True", "stemming": "False", "remove_stopwords": "True", "test_number": "5"}}, {"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "743", "f-measure": "0.584677419355", "recall": "0.877458396369", "precision": "0.438397581255", "tp": "580", "tn": "1901", "fn": "81", "accuracy": "0.750680786687"}}, {"genre": "MPB", "metrics": {"fp": "139", "f-measure": "0.548094373866", "recall": "0.456883509834", "precision": "0.684807256236", "tp": "302", "tn": "2505", "fn": "359", "accuracy": "0.849319213313"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "134", "f-measure": "0.66048862679", "recall": "0.593040847201", "precision": "0.745247148289", "tp": "392", "tn": "2510", "fn": "269", "accuracy": "0.878063540091"}}, {"genre": "Funk Carioca", "metrics": {"fp": "47", "f-measure": "0.838392124692", "recall": "0.773071104387", "precision": "0.915770609319", "tp": "511", "tn": "2597", "fn": "150", "accuracy": "0.940393343419"}}, {"genre": "Samba", "metrics": {"fp": "128", "f-measure": "0.5885509839", "recall": "0.497730711044", "precision": "0.719912472648", "tp": "329", "tn": "2516", "fn": "332", "accuracy": "0.860816944024"}}], "classifier_metrics": [{"macro": {"fp": "238.2", "f-measure": "0.644040705721", "recall": "0.639636913767", "precision": "0.700827013549", "tp": "422.8", "tn": "2405.8", "fn": "238.2", "accuracy": "0.855854765507"}}, {"micro": {"fp": "1191.0", "f-measure": "0.639636913767", "recall": "0.639636913767", "precision": "0.639636913767", "tp": "2114.0", "tn": "12029.0", "fn": "1191.0", "accuracy": "0.855854765507"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "True", "stemming": "False", "remove_stopwords": "False", "test_number": "6"}}, {"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "740", "f-measure": "0.584133400707", "recall": "0.874432677761", "precision": "0.438543247344", "tp": "578", "tn": "1904", "fn": "83", "accuracy": "0.750983358548"}}, {"genre": "MPB", "metrics": {"fp": "150", "f-measure": "0.53742110009", "recall": "0.450832072617", "precision": "0.665178571429", "tp": "298", "tn": "2494", "fn": "363", "accuracy": "0.844780635401"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "129", "f-measure": "0.65072587532", "recall": "0.576399394856", "precision": "0.747058823529", "tp": "381", "tn": "2515", "fn": "280", "accuracy": "0.876248108926"}}, {"genre": "Funk Carioca", "metrics": {"fp": "51", "f-measure": "0.844155844156", "recall": "0.786686838124", "precision": "0.910683012259", "tp": "520", "tn": "2593", "fn": "141", "accuracy": "0.941906202723"}}, {"genre": "Samba", "metrics": {"fp": "133", "f-measure": "0.580875781948", "recall": "0.491679273828", "precision": "0.7096069869", "tp": "325", "tn": "2511", "fn": "336", "accuracy": "0.858093797277"}}], "classifier_metrics": [{"macro": {"fp": "240.6", "f-measure": "0.639462400444", "recall": "0.636006051437", "precision": "0.694214128292", "tp": "420.4", "tn": "2403.4", "fn": "240.6", "accuracy": "0.854402420575"}}, {"micro": {"fp": "1203.0", "f-measure": "0.636006051437", "recall": "0.636006051437", "precision": "0.636006051437", "tp": "2102.0", "tn": "12017.0", "fn": "1203.0", "accuracy": "0.854402420575"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "False", "stemming": "False", "remove_stopwords": "True", "test_number": "7"}}, {"genres_metrics": [{"genre": "Sertanejo", "metrics": {"fp": "775", "f-measure": "0.576808721506", "recall": "0.880484114977", "precision": "0.42888725129", "tp": "582", "tn": "1869", "fn": "79", "accuracy": "0.741603630862"}}, {"genre": "MPB", "metrics": {"fp": "143", "f-measure": "0.531506849315", "recall": "0.440242057489", "precision": "0.670506912442", "tp": "291", "tn": "2501", "fn": "370", "accuracy": "0.844780635401"}}, {"genre": "Ax\u00e9", "metrics": {"fp": "120", "f-measure": "0.649956784788", "recall": "0.568835098336", "precision": "0.758064516129", "tp": "376", "tn": "2524", "fn": "285", "accuracy": "0.877458396369"}}, {"genre": "Funk Carioca", "metrics": {"fp": "53", "f-measure": "0.837133550489", "recall": "0.7776096823", "precision": "0.906525573192", "tp": "514", "tn": "2591", "fn": "147", "accuracy": "0.939485627837"}}, {"genre": "Samba", "metrics": {"fp": "128", "f-measure": "0.580935251799", "recall": "0.488653555219", "precision": "0.716186252772", "tp": "323", "tn": "2516", "fn": "338", "accuracy": "0.859001512859"}}], "classifier_metrics": [{"macro": {"fp": "243.8", "f-measure": "0.635268231579", "recall": "0.631164901664", "precision": "0.696034101165", "tp": "417.2", "tn": "2400.2", "fn": "243.8", "accuracy": "0.852465960666"}}, {"micro": {"fp": "1219.0", "f-measure": "0.631164901664", "recall": "0.631164901664", "precision": "0.631164901664", "tp": "2086.0", "tn": "12001.0", "fn": "1219.0", "accuracy": "0.852465960666"}}], "test_details": {"clf": "NAIVE BAYES DEFAULT", "feature": "BINARY", "case-folding": "False", "stemming": "False", "remove_stopwords": "False", "test_number": "8"}}]